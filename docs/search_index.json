[["index.html", "Littalics in wondeRland Welcome", " Littalics in wondeRland Littal Shemer Haim 2023-02-06 Welcome . "],["introduction.html", "1 Introduction", " 1 Introduction “…and what is the use of a book,” thought Alice, “without pictures or conversations?” L. Carroll, Alice’s Adventures in Wonderland But this book is entirely made up of conversation, and for better conversation! It is targeted at HR leaders who practice or aspire to practice People Analytics and intend to do so by having better conversations with data scientists. It offers my experience as a data scientist in discussions with HR leaders I supported on their journey to data-driven organizations. Inspired by the famous book Alice’s Adventure in Wonderland, I gathered many pieces of knowledge that comprise my story. My career was always on the spectrum between people and businesses, and data mediates between these two poles. The use cases I present will show you how. In addition, it will support your learning in my training programs or others, leveraging available datasets that demonstrate how you communicate with a data scientist who happens to use R and RStudio. This book is unique in the domain of People Analytics because it interwinds two well-known conceptual models: “Employee Lifetime Value” and “Analytics Maturity.” Combining both conceptual models enables a thorough understanding of real People Analytics projects. Furthermore, I did not create the simulations to make HR professionals become data scientists but rather to train them to work with data scientists. Therefore, each use case follows similar steps that enable simulating a conversation with a data scientist. “Oh, I’ve had such a curious dream!” said Alice, and she told her sister, as well as she could remember them, all these strange adventures… But our adventure is certainly not a dream! Let’s Start. "],["why-littalics-in-wonderland.html", "1.1 Why Littalics in wondeRland?", " 1.1 Why Littalics in wondeRland? I’ve always been an autodidact. In my +25 years of professional experience, I was an eternal student in many fields: data science, business, technology, consultancy, writing, photography, and more. However, nothing prepared me for the wonder of learning and mastering R. I took my first steps into R programming in 2015 and never quit. I used R to explore various research and methodologies and leveraged it in my consultancy in many creative ways. At this point in my journey, I needed to organize my resources and experiences in R to be more productive, in the projects and workshops I offer. Inspired by the famous book Alice’s Adventure in Wonderland, I gathered many pieces of knowledge that comprise my story and shared them in an open-book format. Alice figured out in the book that “It’s no use going back to yesterday” because she was a different person back then. However, connecting yesterday’s dots brought me to the person I am today, professionally and personally. Returning to my archive was a way to combine my learning dots into solid content that represents me today. The nickname Littalics is another representation of my connected dots. It started as Littalics on Twitter and Facebook. This nickname referred to Analytics, Statistics, Infographics, Ethics, and all other hashtags ended with the suffix ‘ics.’ Eventually, people recognized me off-line with that name. So, Littalics became a domain name and a business identity. The title of this book, Littalics in wondeRland, implies that it is about my journey in the field and my attempt to organize and share the complex body of knowledge that serves me. But before we take the deep dive into R, I’d like to share some context on the domain of expertise in which I used R. "],["why-people-analytics.html", "1.2 Why People Analytics", " 1.2 Why People Analytics The practice of data science is multidisciplinary. It encompasses three general skills – the business domain of expertise, statistical modeling, and programming. Although I used the term Applied Research for most of my career to describe my practice, I experienced the complexity of the data science profession. I have been a consultant in Organizational Research for more than twenty years, long before “People Analytics” emerged. I offered insights into employee experience, performance measures, collaboration, internal customer satisfaction, safety climate, training assessments, etc. To do so, I had to leverage multidisciplinary skills. I considered my practice a career on a spectrum between people and businesses. Data mediates between these two poles because you reveal every action or transaction between people and companies through the data. I had to be an expert in many organizational use cases while analyzing them by the proper methodology and write my analysis as code for reproducibility, which contributed to the profitability of my small business. Therefore, I was a data scientist, and my projects, which served executives in Human resources, were People Analytics projects, i.e., the data science of HR. People Analytics is still a new and growing field. As such, it encounters obstacles and barriers. I think that there is a vicious circle in this field. On the one hand, data literacy among HR professionals is insufficient. On the other hand, there is inadequate open data and use case demos for learning. I developed learning programs in People Analytics to overcome this vicious circle in recent years. I leveraged the handful of open data sets and created learning materials that included domain expertise, research methodology, and code. I incorporated most of these learning materials in this book. Though powerful, the use of R in HR is surprisingly uncommon. So before diving into People Analytics use cases, my choice to use R and my source of inspiration in open-source culture is worth explaining. "],["why-r-and-rstudio.html", "1.3 Why R and Rstudio?", " 1.3 Why R and Rstudio? I have used SPSS and other statistics and analytics software for over twenty years. Then, one day, a former client mentioned that I should have abandoned them and started using R. Curious and confused, I googled it. Then I downloaded R for free and followed recommendations to use RStudio as my programming environment (IDE), again for free. Next, I searched for some additional resources and thought I should give it a try. It took me only a few weeks to discover huge advantages. First, I was happy to discover that my learning was fast and pleasant. My background in statistics and programming made it relatively easy for me to adapt. Furthermore, the endless resources I found online were a significant tailwind. Indeed, I owe much of my progress to R communities and the open-source culture. When I searched for advanced learning programs and more structured learning paths, I could choose from various affordable programs online created by leading universities and professional experts. Secondly, I could do anything I knew and embrace new tricks quickly and completely free. Soon enough, I discovered the vast ecosystem of libraries. I could pick the right ones for any use, from data manipulation to visualization to modeling to reporting. Fortunately, so many brilliant players in the R ecosystem continue contributing as you read those lines, making data science practices more effortless and productive. I must admit that I was lucky to start my R journey when most of the libraries in R that comprise the Tidyverse had been launched already. Finally, R is a statistical programming language created by scientists for scientists. Therefore, many of its solutions also fit many use cases of applied research in a business context. However, in the business environment, most people are non-software experts. Fortunately, internal data clients can use R scripts and projects as is or with their other analytics and reporting tools in their organization. The easy and quick adoption, the flexible and productive usage, and the suit to the business environment encouraged me to combine R with my professional practice and business activity. "],["combining-it-all.html", "1.4 Combining it all", " 1.4 Combining it all This open-book attempts to integrate my personal career experiences with People Analytics use cases and data science practices. To realize this integration, I followed two well-known conceptual models that served me on my journey: the first model is “Employee Lifetime Value,” and the second is “Analytics Maturity.” I came up with the idea to bind these two conceptual models during the workshops I offered in People Analytics. I wanted to enable HR professionals to impact the business by raising the right business questions and leveraging findings and insights derived from people’s data. But HR professionals can support business decision-making only when they communicate those questions to data scientists. I tried to encourage HR professionals to be proactive in conversations with the data scientist who supports their work. Such conversations mediate data science and the business needs in workforce-related analysis and yield impactful storytelling with data. Therefore, I created simulations for them to enable upskilling and reskilling in analytical mindset and critical thinking without becoming data professionals. Each simulation I created included a workforce use case at some point in the employee lifecycle. It also leveraged the use case to demonstrate a practice or methods in data science. Eventually, combining points on the employee lifecycle and data science practices offered a thorough understanding of real People Analytics projects. The main section of this book (Part 3) includes those simulations. It contains eight chapters; each depicts a People Analytics use case and explores a relevant topic in data science. The structure of the eight chapters covers phases of the two conceptual models, “Employee Lifetime Value” and “Analytics Maturity, and essentially interwinds them. From my experience, the best way to develop HR professionals’ data literacy is by using HR data. Therefore, I used open data sources similar to organizational data sets in most learning programs. However, the available open data on HR topics limited my endeavor. Nevertheless, I gathered data resources covering many stages in the employee lifecycle and used them in all analytics maturity levels after anonymizing and randomizing them. The concept of “Employee Lifetime Value” is reviewed in the 1st module and serves as a basis for understanding the analytics processes and data preparation. To proceed with data exploration, visualizations, and hypotheses, I used a dataset of Employee Absenteeism in the 2nd module. Next, testing hypotheses with ANOVA and linear regression is demonstrated in the 3rd module, leveraging the use case of the Gender Pay Gap. I dedicated the two subsequent modules to advanced analytics: In module 4, I used Performance Measures to demonstrate data reduction with factor analysis. In module 5, I predicted Employee Attrition with logistic regression. The following two modules describe semantic analytics: understanding meaning and social context. In module 6, I used Exit Surveys data to analyze text and Categorical data. In module 7, I explored team collaboration with Organization Network Analysis. The last module is an exception. It does not bring a use case or analytics. Instead, I wrapped up with a review of the future of People Analytics, in which such use cases are automated. In the automation phase, new Ethics considerations become crucial. However, understanding the practical foundations of ethics is a learning topic that, in my opinion, should be seeded when starting the People Analytics journey. As mentioned, I did not create the simulations to make HR professionals become data scientists but rather to train them to work with data scientists. Therefore, each chapter that includes a People Analytics use case follows similar steps that enable simulating conversation with a data scientist that supports HR work: use case description, data source, HR briefing, analytics methods review, analysis using R, storytelling with data, and conclusions. The best use of each chapter is to follow these steps to generalize them in a real-life situation where a data scientist is providing an analytics project. This book is not a textbook about R programming. Instead, it focuses on applying R programming in various use cases in People Analytics. Therefore, I did not cover the basics of R programming and focused only on reviewing R code relevant to those use cases. Frankly, I don’t see any point in repeating the best R open books that serve me as frequent learning materials. However, when using various analytics, I referred to other R available books where the reader can expand on the topic. Nevertheless, if you are new to R, I organized all the references for getting started in the next section to ensure that you are ready - or R ready. However, suppose you are a people scientist or analyst; leverage the use cases to expand your tool kit to use R and, if you already do so, to have new ideas about additional use cases and research solutions. "],["r-you-ready-getting-started.html", "2 R you ready? Getting Started", " 2 R you ready? Getting Started My assumption about this book’s readers is that they are either HR professionals or people analysts. HR professionals may want to leverage the use cases of the book to improve their collaboration with data scientists that support their work. If you are amongst this group, please feel free to skip this part or read it to dismiss the mystery of a data scientist’s desktop. People Analysts may want to expand their tool kit with relevant use cases. If you are amongst this group and have short or no experience with R or RStudio, use the following instructions and resources to get ready to start your journey working with these tools. However, remember that there are other environments and programming languages you can choose. The debate about the most suitable tool for data science or People Analytics is beyond the scope of this book. Instead, I offer some orientation for the tools I use. To get started, I believe you need to follow four steps, which I include in this part. You can’t use R and Rstudio without installing them first. Find the resources for Installation in the first section. Then, writing your first lines of code, you must understand how to navigate and find your path. I dedicate the second section to Navigation in this verse and reference resources for R code, R Packages, the IDE, and additional ways to get help. The following section describes the Analytics Process, and its objective is to help you to integrate the use cases in the book. Lastly, I dedicated some paragraphs to reproducibility because I find that professionals in the field lack an understanding of its importance. However, Reproducibility is inherent to all three previous steps. As Alice thought, she was never sure “what I’m going to be, from one minute to another!” but her analytical mindset helped her to understand that she remains herself, even when changing her size to get into her next adventure. "],["instalation.html", "2.1 Instalation", " 2.1 Instalation There are only three things you need to run the code in this book, and you can get them all for free: R, RStudio, and some R packages. First, install R. To download R, go to CRAN (acronyms of the Comprehensive R Archive Network) and follow the instruction. When you finish installation, you’ll have the R programming language on your device. Then, you can program and run the R code independently or in other computer programs, like Rstudio. Secondly, download and install RStudio, the IDE (acronyms of integrated development environment) for R programming. Note that RStudio is not functional without installing R first. In the next section of this part of the book, Navigation, we’ll look at this environment, where you write your R code, run it, and manage your working environment. Lastly, you’ll need to install some R packages, which include functions, datasets, and documentation that expands base R functionality. There are more than 18,000 packages, so which ones do you need? It depends on your domain, analysis, and preferences. In the Navigation section, I review some packages I used in this book. You’ll find additional information in the code within each use case of the book’s third part. Are these installations a one-time hassle? Probably not. R, Rstudio, and R packages frequently update as all things software. A new major R version comes out yearly, with 2-3 minor releases. In addition, R packages are updated from time to time. When you upgrade to a major R version, it requires reinstalling all your packages. RStudio is also updated a few times a year. Please check the colophon in the appendix if you are curious about the versions of R, Rstudio, and the packages I used when I wrote codes for this book. In case you need a comprehensive step-by-step guide for installing R and RStudio, I believe you’ll find the right one within millions of results on Youtube or Google search. For example, many universities have published their tutorials. However, I refer to one resource: Follow the introduction in the book R for Data Science by Hadley Wickham and Garrett Grolemund, which is a valuable resource along your way, whatever it may be. "],["navigation.html", "2.2 Navigation", " 2.2 Navigation If you take your first steps in wondeRland, you must have a few navigation tools. First, you must be acquainted with the basic R syntax. Then, you better understand how to leverage the basic R code with functions of the suitable packages. In addition, you need to organize your data, scripts, outputs, and other resources in your development environment. Lastly, you should always know where to get help when you need it. This section lists all those navigation tools but does not offer a tutorial for any of them. Instead, it refers to some other open books. However, there are endless resources online, including tutorials, articles, and posts on online forums. So when you go on the adventure of writing code, Google is always your best companion. But remember to add “R” to your search query to restrict it to relevant results. 2.2.1 R code I think that this is the boring part of your journey. By itself, programming is not enjoyable for most professionals in the domain of people. However, dirtying my hands in writing code enabled me to tackle some challenges, as you will read in the use cases in the book’s third part. You can screen into the R code to see how I approached them. Your motivation to do so may be better communication with the data scientist that supports your work. Like all programming languages, R has its syntax. But its grammar is relatively easy, and its documentation can help you follow the steps of the data science processes. Therefore, I believe your acquaintance with it will support your interactions with data professionals. In his Handbook of Regression Modeling in People Analytics, Keith McNulty offered an excellent review of R. It covers the basics of R, allowing the inexperienced reader to have some orientation. This review explains all the basics you need: data types and structures in R; the basics of functions and visualizations; and how to handle errors and warnings messages. 2.2.2 R Packages The R programming language is open-source, i.e., people contribute free packages (currently more than 18,000) that make R so powerful, enabling users to do anything in it. Each R package contains functions and documentation for a specific use. It solves a particular problem for the user. Furthermore, all Packages are standardized to allow you to install and use them in your programming environment. To demonstrate installing and loading R packages and to dismiss the confusion about R packages, Ismay and Kim, the authors of the book Statistical Inference via Data Science, used the analogy of apps and mobile phones. R is like a new mobile phone, which has a few features when you use it for the first time. But it doesn’t have everything the user wants or needs. So, R packages are just like apps users can download onto their mobile. Most packages are not installed by default when R and RStudio are installed. So to use the R package for the first time, the user must install it first. Then the user needs to load it each time RSudio is started, just like opening an app on a mobile phone. Similar to apps on mobile phones, downloading and loading R packages is not the end of the story because the user needs to download updates from time to time. 2.2.3 Your IDE When I present screenshots of my work in R-Studio to my workshop participants, I have a single objective: dismissing the mystery of a data scientist’s desktop. If more people become keen users of this IDE, I would consider it a pleasant side effect of my endeavor to enhance communication between HR professionals and date professionals. To explain what I do on R-Studio, I use the analogy of a cake recipe. A good recipe includes a list of ingredients, precise instructions on how to use them, a description of the expected outcomes, and a pleasant and delicious visual display. Of course, R-Studio includes so much more, but let’s stick to the analysis recipe for simplicity. This paragraph is not a tutorial for R-Studio, but rather a general orientation in your navigation. Therefore I mention only some windows in R-Studio, which I picked according to their relevance to the analysis recipe: The environment window includes the ingredients of the recipe: datasets you have opened, new variables you have created, and any other defined objects you make and use during cooking. The script window includes tabs of code in different formats. These code tabs serve as the exact cooking instructions. In addition, they enable you to run, document, and repeat the analysis. Consider the console window a fast oven, where the code runs and outputs are created. The final result may be presented visually as a delicious display in the viewer window. Explore R-Studio’s endless learning resources if you want to learn it from scratch. Otherwise, the analysis recipe is completed and served in the book’s third part. Bon-appetite! 2.2.4 Getting Help Whenever I got stuck using R, Google was my friend. Every error message I googled eventually revealed a resource for resolution because I was not the first to tackle it. However, if you search for help on Google, keep in mind two things: First, you must add “R” to your query to ensure the results are relevant. Secondly, if the search isn’t helpful, it may mean your question is not precise enough, so you should try rephrasing it. Sometimes your Google search results include Stackoverflow questions. The questions and answers on this platform may consist of reproducible examples. In such examples, following the code and data is usually the quick way to resolve your issues. In addition, votes on questions and answers are helpful. Other times Google search results include articles from R-bloggers. This website aggregates hundreds of blogs about R. Some blogs may contain the answers you need. But it is only one resource of the vast community of R users and developers. I recommend following other activities of the R community to broaden your perspective on the problems you face. However, before searching for help outside your IDE, I suggest you try to help yourself with R documentation and help tools. Navigating function documentation is a must because there is no way to remember all parameters and options to twig a function. Essentially it is also hard to remember all the functions in each library. Therefore I recommend getting to know and using RStudio cheatsheets. "],["analytics-process.html", "2.3 Analytics Process", " 2.3 Analytics Process People Analytics is all about exploring, inferring, and communicating data patterns. The objective is to support decisions related to people in the organization. Therefore, it is essential to understand how analytics serves as a phase in the decision-making process. To do so, excuse my nostalgia; I browse my graduation project written in 1995 to quote the DECIDE acronym. The DECIDE model is a framework developed by marketing researchers, which I found in an early edition of Malhotra’s book: “Marketing Research – An Applied Orientation” (Prentice Hall). Although I originally learned it in a different context of business research, it is undoubtedly applicable to the domain of people. The acronym DECIDE may stand for: Define the problem, Explore the factors, Choose research design, Investigate and analyze, Draw conclusions and recommendations, and Evaluate action effectiveness. Research is a loop process because the evaluation phase at its end leads to new problem definitions, therefore, a new or repeated study. The acronym DECIDE is essential because it emphasizes that every study in the business realm must start with a question in mind and end with actionable insights. The domain of people is not an exception. Defining the research objectives and drawing conclusions and recommendations are essentially the role of the research sponsor in the HR department. In a sense, it is the casing of the data scientist’s role in the analysis phase. Therefore, when communicating with a data scientist to present the research objectives before the analysis phase or to extract study results for conclusions and recommendations, it is helpful to understand the analytics process from the point of view of the data scientist. This process includes five action steps: Importing and Cleaning; Manipulating and Transforming; Exploring and Visualizing; Analyzing and Modeling; Communicating and Reporting. So, let’s delve into these steps. 2.3.1 Importing and Cleaning Data import and cleaning are essential in analytics, laying the foundation for all subsequent phases. Ensuring that the data is accurate and ready for analysis avoids errors later in the process. This step involves bringing the data into the project environment and preparing it for analysis. It includes tasks such as: importing the data from its source, checking the data quality by identifying and filling in missing data, spotting invalid records and correcting errors, and removing duplicates. This step also includes structuring the data to make it appropriate for work and organizing it in a standard way, called tidy data. This standard format has three key characteristics: each column represents a variable, each row represents a unique observation, and each value is the intersection of variable and observation. Tidy data allows data scientists to focus on the core analysis rather than getting bogged down in data preparation. Here are some of the most useful libraries available for importing and cleaning data for People Analysts who take their first steps in R: I suggest covering the documentation and cheatsheets of these libraries. readr functions for reading and writing flat files, such as CSV tidyr functions for cleaning and reshaping data in a tidy format dplyr functions for manipulating and summarizing data 2.3.2 Manipulating and Transforming Data manipulation and transformation are vital in every data science project because they prepare the data for analysis according to the research objectives and enable the revealing of patterns and relationships in the data later in the project. This step involves modifying and rearranging the data to make it more suitable for analysis and modeling. It includes tasks such as grouping and summarizing records, selecting and filtering subsets based on specific criteria, and transforming the data to scale or normalize it. One fundamental transformation is related to the data format, i.e., its structure: “long” or “wide.” For example, suppose that each record in a dataset represents a group whose identity is recorded in the first column. In a wide format, the first column contains unique values. But in a long format, the first column contains values that may repeat, so the information of a specific group may be recorded in several rows. Both “long” and “wide” formats include the same information but in a different structure. Most datasets you’ll encounter in People Analytics are probably in a wide format. However, for certain functions, e.g., when visualizing data, you’ll have to convert the data to a long format. An essential part of transforming the data is manipulating variables according to their particular classes, e.g., dates, text, or categorical variables. Therefore, in addition to the R libraries mentioned earlier, it is worth exploring the following libraries in the context of data transformations. Again, I recommend covering the documentation and cheatsheets of these libraries while remembering there are many other libraries to explore. stringr functions for working with strings (character variables), e.g., searching for patterns, extracting substrings, and manipulations. lubridate functions for working with dates and times, e.g., parsing from strings, extracting components, and calculations. forcats functions for working with factors (categorical variables), e.g., reordering factor levels, collapsing levels, and more. These libraries simplify data scientists’ workflow when transforming non-numeric variables, which are very common in People Analytics. 2.3.3 Exploring and Visualizing Exploring and visualizing the data first, before modeling or continuing with advanced analytics, is an essential step in any project. It enables the data scientist to gain fundamental insights about patterns, factors, relationships, or trends that may be essential for the project’s goals. Such understanding will be later helpful for informed decisions about further analysis, modeling methods, and potential challenges. There are typically two primary techniques to explore the data: plotting it and examining its summary statistics. The outputs of these techniques enable us to understand the distributions in the data, including centers, dispersions, skewness, and outliers. Understanding distributions is crucial at this stage because the distributions of variables significantly impact any analysis, particularly in choosing the appropriate modeling techniques. In addition, this stage is also an essential step in communication at an early stage of the project. The research sponsors usually expect to receive some findings early in the project. By sharing some informative visualizations and information about trends, the data scientist can effectively lower the pressure from the research stakeholders, keep them involved and sometimes get more directions based on the implications of the data exploration for further analysis. In the case of continuous variables, the fundamental charts commonly used in a project’s exploration phase are histograms, boxplots, and scatterplots. They are suitable for exploration because they enable one to grasp the distributions well. Bar charts are also typical for exploring categorical variables. I included these charts in the following use cases presented in the book, but as you will see, there are many other types of charts to choose from, depending on the nature of the data. There are many helpful R libraries for data visualization. But the most fundamental is ggplot2. This famous library provides a wide range of chart types and features, and it is particularly well-suited for creating complex plots with multiple aesthetic options. Many other libraries for visualization are based on it to provide additional customizations. The underlying philosophy of ggplot2 is The Grammar of Graphics, a conceptual framework that became a standard approach to data visualization. The idea is that statistical graphics can be spoken as a language, with specific grammar rules for combining different plot elements to create a compelling and informative visualization. The four components of statistical graphics are data, aesthetic mappings (axis and attributes), geoms (data representations), and additional statistical transformations (e.g., binning, smoothing). The Grammar of Graphics that ggplot2 uses is described in detail in the book ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham et al.  2.3.4 Analyzing and Modeling Analyzing and modeling refer to selecting, using, and validating mathematical or statistical tools and algorithms to infer or predict based on the data. While analyzing and modeling data may be perceived as the essential phase in any analytics process, it is crucial to understand two conditions for its success: First, the data scientist needs to have a question in mind, and second, success depends on the former phases of the process. At this point, I’d like to remind you that your proactivity as an HR professional in the conversation and collaboration with the data scientist is invaluable to the project’s success because it enables both raising the relevant questions and access to appropriate and maintained datasets. Having a straightforward question in mind when analyzing and modeling data is vital for two main reasons: First, it helps to focus the analysis and modeling efforts and avoid getting lost or wasting time on irrelevant analyses or models. A well-defined question guides the selection of appropriate methods for the research objective, for example, comparing groups, exploring trends, or predicting a specific outcome. Secondly, a question in mind allows you to frame the findings and results in the business context, which make it easier to communicate the insights and recommendations to research sponsors and decision-makers. This business context is also relevant to discuss the research limitations, constraints, and potential biases. When we are ready with a question in mind and enthusiast to start analyzing and modeling, we should have already imported and cleaned the data, transformed and manipulated it to make it ready to work with, and gained a pretty good orientation after exploring and visualizing it. So now, we should pick a few mathematical and statistical tools and algorithms out of enormous options. These tools will enable us to infer insights and make predictions, but only if we pick the right ones and use them on suitable datasets we prepared in advance. An important distinction to make here is between inferential statistics and predictive analytics. Both approaches are frequently used, but they have a critical difference: Inferential statistics enables one to test hypotheses and infer about a population based on a sample. In contrast, predictive analytics allows modeling data patterns to make predictions about new and unseen data. Each approach uses different assumptions about the data types and distributions and has different ways to evaluate findings and results. In the use cases in the following sections, I’ll relate to these assumptions and evaluations based on the tool used. Both inferential statistics and predictive analytics have so many R libraries that this book will be too short of mentioning them all. Frankly, I know only a handsome out of the thousands of packages. Still, beyond Tidyverse libraries discussed earlier that are relevant to all use cases, I’ll mention suggested libraries that may be a good start in each one. However, note that R programming language evolves constantly, and so do we. Therefore, your library selection may change over time. Nevertheless, your selected libraries will always depend on the research objectives and the relevant analysis or modeling tools. 2.3.5 Communicating and Reporting The final phase of the analytics process is communicating the results. However, this is the final phase only from the point of view of a data scientist. The reporting tools and contents will serve the research sponsor for conclusions and recommendations that will be a basis for discussions and decision-making. The primary goal of this phase is to clearly and effectively communicate the main findings and insights and to provide guidance for using these findings and insights for informed business decisions. Therefore, the data scientist’s deliverables should be designed as a friendly tool that is easily operated and understood by non-technical audiences. A friendly tool is insufficient, even if it is most visually appealing. Therefore, any communication and reporting format must include A clear explanation of the following: the analysis goal, the methods used to collect and analyze the data, the results and their accuracy, a summary that highlights the main findings and conclusions, and references for external sources. There are many ways to communicate and report findings at the end of the analytics process. Although R scripts are included in many data visualization tools, such as interactive dashboards, I’ll mention two other formats which are congruent to this book: R Markdown: A powerful tool that combines text, code, and output (including visualization) in one document and is used to create reports, presentations, and even websites. The book R Markdown Cookbook by Yihui Xie et al. is a comprehensive guide for this tool. Shiny: A web application framework for R that enables the creation of interactive web apps to share results. When you cover the use cases section of the book, you’ll notice that the complete steps in each use case were written in R code. Of course, it is not always the case since data scientists may use other programming languages or data analysis platforms. However, beyond the advantages of R that I shared in the introduction, one that deserves special attention is its inherited reproducibility, a term I will describe next. "],["reproducibility.html", "2.4 Reproducibility", " 2.4 Reproducibility No matter how you analyze workforce data, your methods will fall into one or more of four categories: Research software, BI tools, Data science programming, and People Analytics products. However, the ability to reproduce your results is not the same in all of them. The concept of reproducibility did not gain enough attention in People Analytics training and practice. In this section, I’ll answer three fundamental questions about reproducibility: What is reproducibility? What aspects of reproducibility are essential in applied research for business, particularly People Analytics? Finally, how does using R programming contribute to research reproducibility? Hopefully, my answers will shed some light on this crucial topic. 2.4.1 What is reproducibility? In general, reproducibility is a central principle in the foundation of the scientific method. Reproducible findings can be reliably achieved when the study is replicated using the same research methodology and analysis. Successful replications of a study are considered scientific knowledge. A reproducible study must provide sufficient detail about its methods, materials, and analysis so that other researchers can recreate the study exactly. Therefore, researchers offer details about their methodology in a typical writing style of scientific articles: research design, sample size, data collection methods, statistical analysis techniques, and more. 2.4.2 Reproducibility in business research In the business context, research reproducibility is somewhat different. The reliability and validity of a study are essential for informed decision-making. If research on people-related questions is not reproducible, it might lead to misleading conclusions, poor decision-making, and negative consequences for the business and its people. In the domain of people in the organization, any research affects various stakeholders: employees, candidates, managers, executives, and even clients, prospects, and society. Reproducibility is essential for building trust and for demonstrating transparency and integrity. The risks of irreproducibility may vary from a bad reputation to legal issues. A research project is an expensive process for the organization. Investing time and resources in irreproducible research means that additional investment is needed when the process runs periodically or when new analysts or data scientists take the role of conducting it. 2.4.3 R programming and reproducibility Reproducibility is relevant in each stage of the research. However, using R can contribute to reproducibility in two specific ways: First, R scripts document the entire data analysis process, including data manipulation, statistical modeling, and visualization, enabling others to replicate the study. Moreover, R scripts are easily shared, reused, and modified. Secondly, the R user community fosters collaboration that contributes to transparency and best reproducibility practices. Furthermore, R packages are fundamental for reproducible R code because they include reusable functions and documentation that describes how to use them. To conclude, reproducibility is essential in business research, particularly in the domain of people. Unfortunately, many HR departments still rely on spreadsheets in their analytics endeavors in practices that lack documentation, so others can’t track or reproduce it. Data scientists who write code in programming languages such as R can leverage access to libraries containing ready-made code and contribute their code documentation. Inherited reproducibility is a significant advantage in using R or other programming languages. "],["people-analytics-methods-and-use-cases.html", "3 People Analytics Methods and Use Cases", " 3 People Analytics Methods and Use Cases I met many HR Leaders and People Analysts over the years, all eager to make an impact in their roles. However, I couldn’t skip noticing how our perspectives differ regarding how you start making an impact. My perspective comes from data science, in which studies aim to discover patterns in data and derive meaningful information to support business decisions. People Analytics, defined as the data science of HR, is all about exploring, inferring, and communicating data patterns to support decisions related to people. To support decisions, always start your analysis with a question in mind. Such a question, handled with the proper analytics process, should lead to actionable insights. If you begin your analytics initiative with data analysis and not a question in mind, there is always a high chance of finding exciting results. However, your results will not affect the business aside from losing valuable managerial attention. A question could be a key concern, a goal, or a challenge for the business. In the case of a People Analytics project, you hypothesize how human performance or behavior impacts that key concern, goal, or challenge. Then you define what you need to measure to test that hypothesis. Having the hypothesis and the measures to test it makes you ready to source the data and start the analytics process. The data in your organization shed light on the business’s current situation and enable an understanding of its factors, directing your intervention and guaranteeing that you discuss your insights in a broader context of the business and workforce markets. However, many HR professionals start elsewhere - with programs, being confident about the organizational development point of view. Even when data is their starting point, it often takes the form of reporting and not exploring. What is the difference between reporting and exploring? To explore data, you must have an analytical mindset. It enables you to analyze information and identify patterns in the data to solve problems. In other words, you use your curiosity by asking the question, “why?”. Dashboards and other reporting methods present different metrics and KPIs and answer the questions: Did we reach our goals? How far are we from achieving our goals? However, by using dashboards, we can’t answer the question, “why?”. So instead, we need to analyze the factors driving those KPIs on our dashboards. I don’t expect HR professionals to become data scientists and run advanced statistics to identify patterns in the data that reveal the factors of KPIs. Still, I’m sure that being a better inner client of data professionals and solutions is essential, and a key to their success is asking, “why?”. It will enable them to tell a straightforward story, impact any topic related to people, track improvement and progress, and indeed contribute and impact the business. Therefore, in the following use cases, I’ll go beyond reporting and dashboarding on the one hand, and I won’t jump to recommended interventions and programs on the other hand. Instead, I will lead the discussion to focus on exploration and demonstrate an analytical mindset that leverages data science in the HR department. The use cases simulate a conversation with a data scientist that supports HR work. Their descriptions include the question in mind, data sources, HR briefing, analytics methods review, analysis using R, storytelling with data, and conclusions. I encourage you to generalize the use cases in real-life situations. "],["employee-lifetime-value.html", "3.1 Employee Lifetime Value", " 3.1 Employee Lifetime Value In this section, we’ll define Employee Lifetime Value and explore how integrating data from different sources and appropriately preparing data has a considerable advantage in analytics. I based this section on my guest lecture at Stanford University’s People Analytics program in June 2022 and a previous article on my blog: Employee Lifetime Value. 3.1.1 The use case Employee Lifetime Value (ELV) refers to the expected value the organization gains in the entire time an employee in a particular role spends working. You probably heard in your career that “people are our most important asset.” It makes sense. People plan and execute the company’s strategy, create competitive advantage, maintain customer relations, and bring innovation to meet future challenges and needs. This statement can be quantified using ELV. ELV is a helpful scheme to present a business case of HR interventions because it knits people processes with business outcomes. HR manages various operations throughout the employee lifecycle: workforce planning, recruitment, onboarding, learning and development, feedback and evaluation, recognition and reward, promotion and internal mobility, employee experience, safety and welfare, and retirement. People processes create aggregated workforce capabilities: engagement, culture, efficiency, leadership, innovation, and so forth. Those capabilities enable the organization to achieve its business goals: productivity, quality, and customer satisfaction, resulting in business outcomes, e.g., revenue growth and stakeholders’ return. The employee’s output varies during his employment in the organization. It is negative initially because of the costs of recruitment, onboarding, and training, and when the employee is still not productive enough. Then, a positive output rises and stabilizes until the employee gets burned out or decides to leave. Therefore, it is insightful, to sum up the net value over time, that is, what the employee produces, subtracting the employment cost, and explore the change as a result of HR intervention. For example, we can explore the impact on the total employee outputs when our interventions shorten the time to productivity, reach a higher result, continue improvement for a longer period, and extend the time that the employee remains in the organization. 3.1.2 Data source The data in this use case is a simulation. However, its structure and sources are based on a real case study from a small company. It contains datasets of two hypothetical recruitment cycles in a factory: The first employee group landed jobs in the year’s first half, and the second group started in the second half. To analyze ELV, we need to integrate each group’s data from different sources. Specifically, some sources include information about employment costs, while others have output records. In real life, we combine data from the core HR platform, payment platform, and records about performance and productivity that the business unit collects. Assuming no People Analytics platform automates data integration from different sources, the first step in this use case would be choosing the relevant records from each platform and combining them all. We should also manually add information about the costs of people processes that may not be included in the platforms mentioned above. 3.1.3 HR briefing Suppose you are early in your journey to establish People Analytics practices as an HR leader in a corporate factory. Your management asked you to advise on low productivity and turnover challenges. You consider leveraging the scheme of ELV as a quick win because you expect it to be impactful and not too complex. The factory recruits periodically in cycles. Employees in each recruitment cycle undergo an extended training plan until they become qualified in the company manufacturing processes. However, the training program may vary in each cycle. Moreover, some improvements in recruitment processes may be introduced between cycles. Different people processes may contribute to productivity loss or employee attrition. Therefore, analyzing the ELV of various recruitment cycles may show the return on investment in each cycle. Your company did not succeed in retaining employees in a particular group for more than a year. So, your team decided to change some processes to deal with this challenge: First, you improved the recruitment process with better and more expensive assessment tools so more suitable candidates would be chosen and eventually contribute to workforce stability and productivity. Then, onboarding and training activities were split and extended for longer, so new employees would reach a higher point of fully contributing earlier in their tenure. You have records of each employee in various recruitment cycles in different platforms, some in your HR department, others in the operation and finance units. You also have information about the costs of various processes that the HR department offers. Therefore, you ask a data scientist who supports your work to combine employee records from different sources into a unified dataset that represents costs and outputs over months and analyze the ELV separately for each recruitment cycle. "],["candidate-evaluation.html", "3.2 Candidate Evaluation", " 3.2 Candidate Evaluation 3.2.1 The use case 3.2.2 Data source The dataset I sourced in this use case was created as a partial dataset of police cadet evaluations, including two thousand observations of cadets’ attributes. (See more information in the dataset appendix). To fit the dataset for this use case, I re-coded some attributes. Eventually, I included the following: demographics at the course start (age, gender, parenthood, criminal record), occupational background and achievements (former academic years, former experience years, course graduation score), and performance score (average evaluation of senior police officers committee). Since my analysis aims to provide actionable insights to a hypothetical HR leader, I’ll go the extra mile beyond visually exploring the differences between successful vs. failing officers, as found in the original publication. I’ll question the impact of attributes on the recruitment process in general. 3.2.3 HR briefing Suppose you are a strategic workforce planer in the Police HR department. A recent challenge in your organization is a high proportion of low evaluations among officers. So you decide to explore how former recruitment processes influence performance. Mainly, you suspect that part of the issue is related to old processes of cadet selection that were neglected to be updated. In addition to data about officers’ evaluations, you have records of police cadets’ attributes that were collected before their course. You also have records of their final course grades. You decide to analyze the associations between the present officers’ evaluation and their former success in the course. Your concern is that the organization failed to select cadets and assign course graduates. Furthermore, you are curious about the evaluation validity. You suspect it might be biased, so you hope to dismiss such concern. Therefore, you ask a data scientist who supports your work to explore the officers’ evaluation to discover its factors and potential biases. You aim to develop insights and recommendations about attributes that may be a better basis for cadet selections and graduation. First, you’d like to better understand the associations by visualizing them. Then, you’d like to point to significant success factors and potential bias. "],["employee-absenteeism.html", "3.3 Employee Absenteeism", " 3.3 Employee Absenteeism In this section, we’ll discuss Employee Absenteeism and leverage the topic to practice exploration and hypotheses set by visualization. I based this section on a previous article on my blog: Visualizing Absenteeism At Work. 3.3.1 The use case Employee Absenteeism is a failure to be or remain at work as scheduled or planned. Excessive absenteeism indicates issues and challenges for the organization and within individuals. Absenteeism causes employers productivity loss. There are two main components for this loss: First, absenteeism reduces the outputs of absent employees and the employees who serve as a replacement. Secondly, there are administrative tasks adjusting the workflow. There are implications for individuals too. First, the absent employee may lose pay and experience decreased performance and harm perception. Secondly, co-workers may experience increased workloads that may end in burnout. In addition, clients may suffer lousy service. The organization can evaluate its absenteeism rate in comparison to the sector or country benchmarks. But such a comparison will not point to the absenteeism causes and the proper intervention to reduce it. However, exploring the associations between absenteeism, employee characteristics, and work characteristics are helpful for actionable insights. 3.3.2 Data source I found the data of this use case in the UC Irvine Machine Learning Repository. The database was created at a courier company in Brazil. It includes records of absenteeism from July 2007 to July 2010. Variables in this dataset encompass time and duration of absence, employee background (distance from residence to work, service time, age, education, social drinking, social smoking), and work characteristics (workload, hit targets, disciplinary failure). The structure of this data set is unique. It contains 740 rows and 20 columns. Each record represents an occurrence of absenteeism due to a single reason, measured in hours. Therefore, each employee may have multiple records marked with the same employee’s ID. These records should be summed up. The dataset was used in academic research at the Universidade Nove de Julho – Postgraduate Program in Informatics and Knowledge Management. The data creators are Andrea Martiniano, Ricardo Pinto Ferreira, and Renato Jose Sassi. A special thanks to my colleagues who wrote the open book HR Analytics in R and brought this data set. However, my approach is different. I aim my analysis towards actionable insights, as if my clients are HR leaders, rather than simply exploring the data for analysis, as my colleagues did. Therefore, all variables are considered predictors in creating the visualizations presented in this use case, while absenteeism is the outcome. 3.3.3 HR briefing Suppose you are an HR leader at a courier company. You prepare to advise an intervention plan for the severe issue of driver absenteeism that costs the company its productivity loss. You have records of absent employees for a couple of years. You also have data about employee background in your HRIS. In addition, the business unit provides you with data about the work characteristics of drivers. You ask a data scientist who supports your work to consider those employee background variables and work characteristics variables as predictors of absenteeism. But first, you want to explore the associations by visualizing them. 3.3.4 Analytics methods The focus of this section is visualization. I visualized the data to explore it, mainly to get a glance into the cause of absenteeism in employee characteristics and work attributes. Obviously, an actual project will include additional multivariate statistics and statistical models, but this is beyond the scope of this section. Visualization is a core skill and activity of a data scientist. To excel in visualizing data, understanding statistics and the proper design are necessary. I neglect the explanations of what makes a good visualization, though, letting the charts speak for themselves. However, in most visualizations, I added some remarks to practice critical thinking, which is not only the responsibility of the data scientist but also his sponsors and audience, aka you. In every analysis process, visualization serves two functions: exploring the data and stewarding it to the audience. In this use case, I tried to do both at once. Therefore, I did not settle for sloppy visualization, which is sufficient for the exploration phase, but I took the effort to make the visualization compelling for reporting. Some of the following visualizations I created may seem complicated at first glance. But when I used composed graphical representations of the data, the objective was to explore, with the fewest charts, the association between absenteeism and its two predictor kinds: employee characteristics and work characteristics. Nevertheless, I kept the story proper, ensuring it was appealing, using the right charts, and emphasizing the importance. 3.3.5 Analysis using R 1 - Absenteeism and Employee Background Does absenteeism in the Brazilian courier company relate to its employees’ background? To be more precise, can we point to specific employee groups more prone to be absent? And maybe intervein among these groups? In this part of the analysis, I explored employee background variables, such as age, tenure, body attributes, social behaviors, and family coincidence. I tried to leverage whatever variables I could find in the data. However, People Analysts who work with actual data in their organizations may discover many more relevant variables. In addition, other variables in this dataset are not typical in organizations. 1.1 Employee Age and Tenure How absenteeism, measured in hours, is associated with employee age or tenure? To explore the relationship between these three numerical variables, I suggested a plot that captures the distribution of each variable, scatters each pair of variables and presents the correlation. As clearly shown in Figure 1, absenteeism is not related to age or tenure in the courier company. Notice, however, that age and tenure are correlated in this organization. It may not be the case among other occupations and organizations. Furthermore, the density plots that give you an impression of the shape of the distribution of each variable may also be unique for this case study. dat.fig1 &lt;- dat %&gt;% select(ID, Absenteeism_time_in_hours, Age, Service_time) %&gt;% group_by(ID) %&gt;% summarize(Absenteeism = sum(Absenteeism_time_in_hours), Age = mean(Age), Tenure = mean(Service_time)) %&gt;% select(Absenteeism, Age, Tenure) my_scatter &lt;- function(data,mapping){ ggplot(data=data, mapping=mapping) + geom_point(color=&quot;#3b93a4&quot;) } my_density &lt;- function(data,mapping){ ggplot(data=data,mapping=mapping) + geom_density(alpha=0.65, fill=&quot;#3b93a4&quot;) } fig1 &lt;- ggpairs(dat.fig1, lower=list(continuous=my_scatter), diag=list(continuous=my_density)) fig1 &lt;- fig1 + labs(title=&quot;Figure 1: Absenteeism, Age and Tenure&quot;) fig1 1.2 Employee Body Attributes Does absenteeism associate with employee weight, height, or body mass index? Not at all, according to Figure 2. It may be nonsense to explore this in the context of an organization, and obviously, you don’t collect such data in most occupations. However, since the data set includes those variables, why not explore them? The following exploration is precisely the same as the previous one. Since I already had the code, I could quickly reproduce the visualization. Notice that some employees in this data set are obese. If we find a positive correlation between absenteeism and weight, it doesn’t necessarily mean obesity causes absenteeism. Correlation does not imply causation. The alternative direction of the relation is possible too: People who tend to be absent gain more weight, for some reason. Either way, it’s not the case here. dat.fig2 &lt;- dat %&gt;% select(ID, Absenteeism_time_in_hours, Weight, Height, Body_mass_index) %&gt;% group_by(ID) %&gt;% summarize(Absenteeism = sum(Absenteeism_time_in_hours), Weight = mean(Weight), Height = mean(Height), BMI = mean(Body_mass_index)) %&gt;% select(Absenteeism, Weight, Height, BMI) fig2 &lt;- ggpairs(dat.fig2, lower=list(continuous=my_scatter), diag=list(continuous=my_density)) fig2 &lt;- fig2 + labs(title=&quot;Figure 2: Absenteeism and employee body attributes&quot;) fig2 1.3 Employee Social Behaviors dat.fig3 &lt;- dat %&gt;% select(ID, Absenteeism_time_in_hours, Social_smoker, Social_drinker) %&gt;% group_by(ID) %&gt;% summarize(Absenteeism = sum(Absenteeism_time_in_hours), Smoking = mean(Social_smoker), Drinking = mean(Social_drinker)) %&gt;% select(Absenteeism, Smoking, Drinking) %&gt;% mutate(Smoking = as.factor(recode(Smoking, `0`=&quot;No&quot;, `1`=&quot;Yes&quot;))) %&gt;% mutate(Drinking = as.factor(recode(Drinking, `0`=&quot;No&quot;, `1`=&quot;Yes&quot;))) fig3.1 &lt;- ggplot(dat.fig3, aes(x=Smoking, y=Absenteeism)) + geom_boxplot(aes(fill=Smoking)) + labs(title=&quot;Figure 3: Absenteeism and Smoking&quot;, x=&quot;Smoking&quot;, y=&quot;Absenteeism (hours)&quot;) + theme(legend.position = &quot;none&quot;) + scale_fill_manual(values=c(&quot;#FFFFFF&quot;, &quot;#3b93a4&quot;)) fig3.2 &lt;- ggplot(dat.fig3, aes(x=Drinking, y=Absenteeism)) + geom_boxplot(aes(fill=Drinking)) + labs(title=&quot;Figure 4: Absenteeism and Drinking&quot;, x=&quot;Drinking&quot;, y=&quot;Absenteeism (hours)&quot;) + theme(legend.position = &quot;none&quot;) + scale_fill_manual(values=c(&quot;#FFFFFF&quot;, &quot;#3b93a4&quot;)) grid.arrange(fig3.1, fig3.2, nrow=1) #testing for independency #table(dat.fig3$Smoking, dat.fig3$Drinking) #chisq.test(table(dat.fig3$Smoking, dat.fig3$Drinking)) 1.4 Employee Family Members dat.fig4a &lt;- dat %&gt;% select(ID, Absenteeism_time_in_hours, Son, Pet) %&gt;% group_by(ID) %&gt;% summarize(Absenteeism = sum(Absenteeism_time_in_hours), Children = mean(Son), Pets = mean(Pet)) %&gt;% mutate(Children = recode_factor(Children, `0`=&quot;No kids&quot;, `1`=&quot;Have kids&quot;, `2` = &quot;Have kids&quot;, `3` = &quot;Have kids&quot;, `4` = &quot;Have kids&quot;)) %&gt;% mutate(Pets = recode_factor(Pets, `0`=&quot;No pets&quot;, `1`=&quot;Have pets&quot;, `2` = &quot;Have pets&quot;, `3` = &quot;Have pets&quot;, `4` = &quot;Have pets&quot;, `5` = &quot;Have pets&quot;, `6` = &quot;Have pets&quot;, `7` = &quot;Have pets&quot;, `8` = &quot;Have pets&quot;,)) %&gt;% select(Absenteeism, Children, Pets) fig4a.1 &lt;- ggplot(dat.fig4a, aes(x=Children, y=Absenteeism)) + geom_boxplot(aes(fill=Children)) + labs(title=&quot;Figure 5: Absenteeism and Children&quot;, x=&quot;Children&quot;, y=&quot;Absenteeism (hours)&quot;) + theme(legend.position = &quot;none&quot;) + scale_fill_manual(values=c(&quot;#FFFFFF&quot;, &quot;#3b93a4&quot;)) fig4a.2 &lt;- ggplot(dat.fig4a, aes(x=Pets, y=Absenteeism)) + geom_boxplot(aes(fill=Pets)) + labs(title=&quot;Figure 6: Absenteeism and Pets&quot;, x=&quot;Pets&quot;, y=&quot;Absenteeism (hours)&quot;) + theme(legend.position = &quot;none&quot;) + scale_fill_manual(values=c(&quot;#FFFFFF&quot;, &quot;#3b93a4&quot;)) grid.arrange(fig4a.1, fig4a.2, nrow=1) #testing for independency #table(dat.fig4a$Children, dat.fig4a$Pets) #chisq.test(table(dat.fig4a$Children, dat.fig4a$Pets)) 2 - Absenteeism and Work Characteristics 2.1 Absenteeism, Workload and Commute Distance dat.fig5 &lt;- dat %&gt;% select(ID, Absenteeism_time_in_hours, Work_load_Average_per_day, Distance_from_Residence_to_Work, Education) %&gt;% group_by(ID) %&gt;% summarize(Absenteeism = sum(Absenteeism_time_in_hours), Workload = mean(Work_load_Average_per_day ), Distance = mean(Distance_from_Residence_to_Work), Education = mean(Education)) %&gt;% mutate(Education = recode_factor(Education, `1`=&quot;Low&quot;, `2`=&quot;Mid&quot;, `3` = &quot;High&quot;)) %&gt;% select(Absenteeism, Workload, Distance, Education) fig5.1a &lt;- ggplot(dat.fig5, aes(x=Workload, y=Absenteeism)) + geom_jitter(size=3, color=&quot;#3b93a4&quot;) + geom_smooth(method = lm, se = FALSE, color=&quot;black&quot;) + labs(title=&quot;Figure 7: Absenteeism &amp; Workload&quot;, x=&quot;Workload&quot;, y=&quot;Absenteeism (hours)&quot;) fig5.2a &lt;- ggplot(dat.fig5, aes(x=Distance, y=Absenteeism)) + geom_jitter(size=3, color=&quot;#3b93a4&quot;) + geom_smooth(method = lm, se = FALSE, color=&quot;black&quot;) + labs(title=&quot;Figure 8: Absenteeism &amp; Commute&quot;, x=&quot;Commute Distance&quot;, y=&quot;Absenteeism (hours)&quot;) grid.arrange(fig5.1a, fig5.2a, nrow=1) fig5.1 &lt;- ggplot(dat.fig5, aes(x=Workload, y=Absenteeism, fill=Education, color=Education)) + geom_jitter(size=3) + geom_smooth(method = lm, se = FALSE, aes(colour=Education)) + scale_color_manual(values=c(&quot;#92A9BD&quot;, &quot;#3b93a4&quot;, &quot;#072227&quot;)) + scale_fill_manual(values=c(&quot;#92A9BD&quot;, &quot;#3b93a4&quot;, &quot;#072227&quot;)) + labs(title=&quot;Figure 9: Absenteeism &amp; Workload&quot;, subtitle=&quot;by Education Level&quot;, x=&quot;Workload&quot;, y=&quot;Absenteeism (hours)&quot; ) + theme(legend.position=&quot;top&quot;) fig5.2 &lt;- ggplot(dat.fig5, aes(x=Distance, y=Absenteeism, fill=Education, color=Education)) + geom_jitter(size=3) + geom_smooth(method = lm, se = FALSE, aes(colour=Education)) + scale_color_manual(values=c(&quot;#92A9BD&quot;, &quot;#3b93a4&quot;, &quot;#072227&quot;)) + scale_fill_manual(values=c(&quot;#92A9BD&quot;, &quot;#3b93a4&quot;, &quot;#072227&quot;)) + labs(title=&quot;Figure 10: Absenteeism &amp; Commute&quot;, subtitle=&quot;by Education Level&quot;, x=&quot;Commute Distance&quot;, y=&quot;Absenteeism (hours)&quot; ) + theme(legend.position=&quot;top&quot;) grid.arrange(fig5.1, fig5.2, nrow=1) 2.2 Absenteeism, Hitting targets and Disciplinary failures dat.fig6 &lt;- dat %&gt;% select(ID, Absenteeism_time_in_hours, Hit_target, Disciplinary_failure) %&gt;% group_by(ID) %&gt;% summarize(Absenteeism = sum(Absenteeism_time_in_hours), Success = mean(Hit_target), Indicipline = mean(Disciplinary_failure)) %&gt;% mutate(AbsenteeismDir = ifelse(Indicipline&gt;0.15, -1*Absenteeism, Absenteeism)) %&gt;% select(AbsenteeismDir, Success, Indicipline) fig6.1 &lt;- ggplot(dat.fig6, aes(x=Success, y=AbsenteeismDir)) + geom_segment( aes(x=Success, xend=Success, y=0, yend=AbsenteeismDir), color=&quot;grey&quot;) + geom_point( color=&quot;#3b93a4&quot;, size=2) + theme_light() + theme( panel.grid.major.x = element_blank(), panel.border = element_blank(), axis.ticks.x = element_blank() ) + labs(title=&quot;Figure 11: Absenteeism and Target Hits&quot;, x=&quot;Hit Target&quot;, y=&quot;Absenteeism (hours)&quot;) + annotate(&quot;text&quot;, x = 90.5, y = 400, label = &quot;Disciplined Employees&quot;, colour=&quot;#3b93a4&quot;, size=5) + annotate(&quot;text&quot;, x = 90.5, y = -200, label = &quot;Undisciplined Employees&quot;, colour=&quot;#3b93a4&quot;, size=5) + annotate(&quot;rect&quot;, xmin = 88, xmax = 98, ymin = -400, ymax = 0, alpha = .1) fig6.1 2.3 When Absenteeism Occures? Months and Seasons dat.fig7 &lt;- dat %&gt;% select(Absenteeism_time_in_hours, Month_of_absence) %&gt;% group_by(Month_of_absence) %&gt;% summarize(Absenteeism = sum(Absenteeism_time_in_hours)) fig7 &lt;- ggplot(dat.fig7, aes(x=Month_of_absence, y=Absenteeism)) + geom_line(color=&quot;#3b93a4&quot;) + geom_point(color=&quot;#3b93a4&quot;, size=3)+ scale_x_discrete(name =&quot;Months&quot;, limits=c(&quot;JAN&quot;,&quot;FEB&quot;,&quot;MAR&quot;,&quot;APR&quot;,&quot;MAY&quot;,&quot;JUN&quot;,&quot;JUL&quot;,&quot;AUG&quot;,&quot;SEP&quot;,&quot;OCT&quot;,&quot;NOV&quot;,&quot;DEC&quot;)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(title=&quot;Figure 12: Monthly Absenteeism&quot;, x=&quot;Month&quot;, y=&quot;Absenteeism (hours)&quot;) dat.fig8 &lt;- dat %&gt;% select(Absenteeism_time_in_hours, Day_of_the_week) %&gt;% group_by(Day_of_the_week) %&gt;% summarize(Absenteeism = sum(Absenteeism_time_in_hours)) fig8 &lt;- ggplot(dat.fig8, aes(x=Day_of_the_week, y=Absenteeism)) + geom_line(color=&quot;#3b93a4&quot;) + geom_point(color=&quot;#3b93a4&quot;, size=3)+ scale_x_discrete(name =&quot;DAYS&quot;, limits=c(&quot;Sun&quot; ,&quot;Mon&quot;,&quot;Tue&quot;,&quot;Wed&quot;,&quot;Thu&quot;,&quot;Fri&quot;, &quot;Sat&quot;)) + ylim(200,1800) + labs(title=&quot;Figure 13: Daily Absenteeism&quot;, x=&quot;Days&quot;, y=&quot;Absenteeism (hours)&quot;) grid.arrange(fig7, fig8, nrow=1) dat.fig9 &lt;- dat %&gt;% select(Absenteeism_time_in_hours, Day_of_the_week, Month_of_absence) %&gt;% group_by(Day_of_the_week, Month_of_absence) %&gt;% summarize(Absenteeism = sum(Absenteeism_time_in_hours)) fig9 &lt;- ggplot(dat.fig9, aes(x=Day_of_the_week, y=Month_of_absence, fill=Absenteeism)) + geom_tile() + scale_fill_gradient(low = &quot;#EDEDED&quot;, high = &quot;#3b93a4&quot;) + #theme_bw() + theme_light() + theme( panel.grid.major.x = element_blank(), panel.border = element_blank(), axis.ticks.x = element_blank() ) + scale_y_discrete(name =&quot;Months&quot;, limits=c(&quot;JAN&quot;,&quot;FEB&quot;,&quot;MAR&quot;,&quot;APR&quot;,&quot;MAY&quot;,&quot;JUN&quot;,&quot;JUL&quot;,&quot;AUG&quot;,&quot;SEP&quot;,&quot;OCT&quot;,&quot;NOV&quot;,&quot;DEC&quot;)) + scale_x_discrete(name =&quot;Days&quot;, limits=c(&quot;Sun&quot; ,&quot;Mon&quot;,&quot;Tue&quot;,&quot;Wed&quot;,&quot;Thu&quot;,&quot;Fri&quot;, &quot;Sat&quot;)) + labs(title=&quot;Figure 14: Absenteeism by Days and Monthes - Heat Map&quot;) + geom_text(aes(label = Absenteeism), color = &quot;white&quot;, size = 4) fig9 3.3.6 Storytelling with data 3.3.7 Conclusions "],["gender-pay-gap.html", "3.4 Gender Pay Gap", " 3.4 Gender Pay Gap In this section, we’ll discuss Gender Pay Gap and leverage the topic to practice hypotheses setting and testing by ANOVA and Linear Regression. I based this section on three articles published on my blog: (1) Gender Pay Gap: Practice People Analytics With Open Data, (2) Finding Hidden Patterns In Gender Pay Gap Data, and (3) Gender Pay Gap: More Hidden Patterns. 3.4.1 The use case The gender pay gap exists despite regulation, policies, hype, and preoccupation with the subject. Women still earn less than men in many countries, sectors, and roles. Moreover, the gap has been persistent over the years. There is no single cause for the gender pay gap. The reasons include: Gender-biased industries and roles in which female-dominated jobs often have lower wages and in which women are under-represented in high-income jobs; Bias and discrimination in recruitment, promotion, and compensation decisions; Disproportionate share of unpaid domestic work and care that women tend to handle more; Women’s careers are impacted by more time spent out of work as parents, and suffer more from the lack work flexibility in senior roles. To address the gender pay gap, employers may intervene in different ways. First, they need to analyze their pay data and processes. Then, after understanding the source of bias, they can take action and review the impact over time. Closing the gender pay gap is essential for everybody, not only women. Employers who take action to close the pay gap may benefit from higher retention, improved employee engagement, higher levels of productivity, and lower legal risks. In addition, the entire society may benefit as women increase their bargaining power and independence and decrease the chance of poverty later in life. 3.4.2 Data source My source and inspiration for this case study was a dataset of employee salaries in a municipal authority organization, Montgomery County, Maryland, in 2017. For public transparency, this organization shares its dataset. The employee salaries in 2017 contained almost 10 thousand records and included annual salary information and some demographics and background variables (gender, tenure, role, department, full and part-time position). However, for our educational purpose, I anonymized and randomized it, so, from the following findings, it would be impossible to point to individuals or even recognize the organization. But I guarantee that the dataset I used is realistic. Before we proceed to the HR briefing, there are some notes worth mentioning about this dataset. First, it includes only binary gender categories, restricting the analysis to comparing only men vs. women. However, some organizations use more gender categories nowadays. Secondly, in many organizations, the data about employees’ occupation, seniority, gender, mode of employment, and salary details are likely to be stored in more than one platform, for example, the HR platform and payroll software. Moreover, there may be more than one payroll system in global organizations, and it is necessary to merge the data and manipulate it for a unified currency. If your platforms are not integrated, you must incorporate data from different sources for your project and make sure that your process is reproducible. Lastly, If you’d look at the CSV file for this demo, you’d immediately see - It is tidy data! It assigns each row to an employee and each column to a variable. Each cell represents only one value specific to one employee and one variable. Of course, the data you pull out for analysis may not be so neat in real-life situations. There will be duplicate records, missing entries, typos, and more. Your data scientist will need to clean and prepare the data for the project. This work may be tedious, but the systematic errors you may find will help you establish better maintenance practices and improve the data over time. It would be helpful beyond best practice because more and more countries regulate the gender pay gap. 3.4.3 HR briefing Suppose you are an HR leader in a municipal authority organization. Like many public organizations, your employer is subjected to strict regulations regarding equal pay. However, only going beyond the basic comparison between men and women will enable you to spot patterns of bias and reach some actionable insights. Therefore, you decide to shed light on the still-existing pay gap, understand its factors, direct your intervention, and guarantee that you discuss insights in a broader context of the business and workforce markets. In other words, you decide to go beyond reporting and go the extra mile to explore the data. What is the difference between reporting and exploring? To explore data, you must have an analytical mindset. It enables you to analyze information and identify patterns in the data to solve problems. So you use your curiosity by asking the question, “why?”. It will enable you to tell a straightforward story, impact any topic related to people, track improvement and progress, and certainly contribute to closing the gender pay gap. You may already have dashboards that enable you to present different metrics and KPIs and answer the questions: Did we reach our goals? How far are we from achieving our goals? However, by using dashboards, you can’t answer the question: Why? Instead, you need to analyze the factors that drive those KPIs presented on your dashboards. Therefore, you will explore the data beyond finding differences between men and women in compensation and explore how those differences occurred, implying what you should do about it. You’ll reach actionable insights by actively working with your data scientist. When you ask “why,” the data scientist will provide answers based on the data. 3.4.4 Analytics methods 3.4.5 Analysis using R par(mfrow = c(2, 2)) hist(Employees$Annual.Salary[Employees$Gender==&quot;F&quot;], col=&quot;orange&quot;, xlim=c(0,300000), breaks = 50, freq = FALSE, xlab = NULL, main = &quot;Current Annual Salary - Women, Avg 73K&quot;) curve(dnorm (x, mean = mean(Employees$Annual.Salary), sd = sd(Employees$Annual.Salary)), add = TRUE) boxplot(Employees$Annual.Salary[Employees$Gender==&quot;F&quot;], horizontal = TRUE, col=&quot;orange&quot;, ylim = c(0,300000)) hist(Employees$Annual.Salary[Employees$Gender==&quot;M&quot;], col=&quot;lightblue&quot;, xlim=c(0,300000), breaks = 50, freq = FALSE, xlab = NULL, main = &quot;Current Annual Salary - Men, Avg 77K&quot;) curve(dnorm (x, mean = mean(Employees$Annual.Salary), sd = sd(Employees$Annual.Salary)), add = TRUE) boxplot(Employees$Annual.Salary[Employees$Gender==&quot;M&quot;], horizontal = TRUE, col=&quot;lightblue&quot;, ylim = c(0,300000)) par(mfrow = c(2, 2)) hist(EmployeesDiverse$Annual.Salary[EmployeesDiverse$Gender==&quot;F&quot;], col=&quot;orange&quot;, xlim=c(0,300000), breaks = 50, freq = FALSE, xlab = NULL, main = &quot;Current Annual Salary - Women, Avg 72K&quot;) curve(dnorm (x, mean = mean(EmployeesDiverse$Annual.Salary), sd = sd(EmployeesDiverse$Annual.Salary)), add = TRUE) boxplot(EmployeesDiverse$Annual.Salary[EmployeesDiverse$Gender==&quot;F&quot;], horizontal = TRUE, col=&quot;orange&quot;, ylim = c(0,300000)) hist(EmployeesDiverse$Annual.Salary[EmployeesDiverse$Gender==&quot;M&quot;], col=&quot;lightblue&quot;, xlim=c(0,300000), breaks = 50, freq = FALSE, xlab = NULL, main = &quot;Current Annual Salary - Men, Avg 78K&quot;) curve(dnorm (x, mean = mean(EmployeesDiverse$Annual.Salary), sd = sd(EmployeesDiverse$Annual.Salary)), add = TRUE) boxplot(EmployeesDiverse$Annual.Salary[EmployeesDiverse$Gender==&quot;M&quot;], horizontal = TRUE, col=&quot;lightblue&quot;, ylim = c(0,300000)) interaction2 &lt;- aggregate(Employees$Annual.Salary, by=list(Employees$Gender, Employees$Tenure), FUN=mean, na.rm=TRUE) colnames(interaction2) &lt;-c(&quot;Gender&quot;, &quot;Tenure&quot;, &quot;Salary&quot;) interaction2plot &lt;- ggplot(data=interaction2, mapping=aes(x=Tenure, y=Salary, color=Gender)) + geom_point(size = 5) + geom_line(aes(group = Gender), size=1) + scale_y_continuous(name=&quot;Annual Salary&quot;, limits=c(30000,100000), labels = scales::comma) + scale_color_manual(values=c(&quot;orange&quot;, &quot;skyblue&quot;)) + theme_bw() interaction2plot interaction2diverse &lt;- aggregate(EmployeesDiverse$Annual.Salary, by=list(EmployeesDiverse$Gender, EmployeesDiverse$Tenure), FUN=mean, na.rm=TRUE) colnames(interaction2diverse) &lt;-c(&quot;Gender&quot;, &quot;Tenure&quot;, &quot;Salary&quot;) interaction2plotdiverse &lt;- ggplot(data=interaction2diverse, mapping=aes(x=Tenure, y=Salary, color=Gender)) + geom_point(size = 5) + geom_line(aes(group = Gender), size=1) + scale_y_continuous(name=&quot;Annual Salary&quot;, limits=c(30000,100000), labels = scales::comma) + scale_color_manual(values=c(&quot;orange&quot;, &quot;skyblue&quot;)) + theme_bw() interaction2plotdiverse reg2plotdiverse &lt;- EmployeesDiverse %&gt;% ggplot(aes(Tenure.Years, Annual.Salary, color=Gender)) + geom_point(alpha = 0.5) + geom_smooth(method = lm, se = FALSE, aes(colour=Gender), size=1.75) + ylim(0,200000) + scale_color_manual(values=c(&quot;orange&quot;, &quot;skyblue&quot;)) + theme_bw() reg2plotdiverse reg2Aplotdiverse &lt;- EmployeesDiverse %&gt;% ggplot(aes(Tenure.Years, Annual.Salary, color=Gender, size=Assignment)) + #Part-time dots enlarged geom_point(alpha = 0.3) + geom_smooth(method = lm, se = FALSE, aes(colour=Gender), size=1.75) + ylim(0,200000) + scale_color_manual(values=c(&quot;orange&quot;, &quot;skyblue&quot;)) + theme_bw() reg2Aplotdiverse reg2Bplotdiverse &lt;- EmployeesDiverse %&gt;% ggplot(aes(Tenure.Years, Annual.Salary, color=Gender, size=Assignment)) + geom_point(alpha = 0.3) + geom_smooth(method = lm, se = FALSE, aes(colour=Gender, linetype=Assignment), size=1.75) + #Part-time line separated ylim(0,200000) + scale_color_manual(values=c(&quot;orange&quot;, &quot;skyblue&quot;)) + theme_bw() reg2Bplotdiverse 3.4.6 Storytelling with data 3.4.7 Conclusions "],["performance-measures.html", "3.5 Performance Measures", " 3.5 Performance Measures (Levels of measures, Factor Analysis) "],["learning-needs.html", "3.6 Learning Needs", " 3.6 Learning Needs "],["employee-attrition.html", "3.7 Employee Attrition", " 3.7 Employee Attrition (Predictive analytics, Logistic Regression) "],["employee-attrition-survival-analysis.html", "3.8 Employee Attrition (Survival Analysis)", " 3.8 Employee Attrition (Survival Analysis) "],["teams-and-collaboration.html", "3.9 Teams and collaboration", " 3.9 Teams and collaboration (Organization Network Analysis) "],["exit-surveys.html", "3.10 Exit Surveys", " 3.10 Exit Surveys (Text and Categorical data) "],["the-future-of-people-analytics.html", "3.11 The future of People Analytics", " 3.11 The future of People Analytics (Ethics considerations) "],["references.html", "4 References", " 4 References "],["r-open-books.html", "4.1 R Open Books", " 4.1 R Open Books Ismay C., Kim A. Y., (2019) Statistical Inference via Data Science: A ModernDive into R and the Tidyverse, CRC Press Kindle Version, Online Open Version McNulty K., (2021) Handbook of Regression Modeling in People Analytics, CRC Press Kindle Version, Online Open Version Wickham H., Grolemund G., (2017) R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, O’Reilly Kindle Version, Online Open Version Wickham H., Navarro D., and Thomas Lin Pedersen T.L., (2016) ggplot2: Elegant Graphics for Data Analysis, Springer Kindle Version, Online Open Version Xie Y., Dervieux C., Riederer E., (2020) R Markdown Cookbook, CRC Press Kindle Version, Online Open Version "],["open-datasets.html", "4.2 Open Datasets", " 4.2 Open Datasets "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
